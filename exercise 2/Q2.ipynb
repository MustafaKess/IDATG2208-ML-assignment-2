{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68f6ae6b",
   "metadata": {},
   "source": [
    "# Exercise-2: Decision Trees \n",
    "### Code preperations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44d5ef06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"../archive/data.csv\")\n",
    "df = df.drop(columns=['id', 'Unnamed: 32'], errors='ignore')\n",
    "\n",
    "X = df.drop(columns=['diagnosis'])\n",
    "y = df['diagnosis'].map({'M':1, 'B':0})\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475c85a8",
   "metadata": {},
   "source": [
    "### Q2.1  Train a Decision Tree classifier using default parameters. Evaluate it on validation sets from original splits (report accuracy mean and std). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "755cd017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9386\n",
      "Mean Accuracy: 0.9386\n",
      "Std Accuracy: 0.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=42) # No specified parameters\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_val_pred = clf.predict(X_val)\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "accuracies = [accuracy]  \n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Mean Accuracy: {np.mean(accuracies):.4f}\")\n",
    "print(f\"Std Accuracy: {np.std(accuracies):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61221c7",
   "metadata": {},
   "source": [
    "## Q2.2 From the trained model, comment on feature importance values and identify the top 3 features from your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "018c9b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 important features:\n",
      "perimeter_worst         0.704881\n",
      "concave points_worst    0.136016\n",
      "texture_worst           0.038957\n",
      "dtype: float64\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "importances = pd.Series(clf.feature_importances_, index=X.columns)\n",
    "\n",
    "print(\"Top 3 important features:\")\n",
    "print(importances.sort_values(ascending=False).head(3))\n",
    "\n",
    "print(\"-------------\")\n",
    "\n",
    "#print(\"All feature importances:\")                   # Can be ignored, just my curiosity\n",
    "#print(importances.sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c658fa",
   "metadata": {},
   "source": [
    "## Q2.3\n",
    "Vary the max depth parameter (e.g., depth 2-10). Use validation accuracy (mean\n",
    "± std from cross-validation on the training set) to choose the best depth. Provide\n",
    "performance for each chosen depth (at-least 5 to be reported) and discuss the aspects\n",
    "of overfitting vs. underfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11359ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth=2: Accuracy=0.9298 ± 0.0309\n",
      "Depth=3: Accuracy=0.9355 ± 0.0236\n",
      "Depth=4: Accuracy=0.9326 ± 0.0343\n",
      "Depth=5: Accuracy=0.9298 ± 0.0309\n",
      "Depth=6: Accuracy=0.9298 ± 0.0395\n",
      "Depth=7: Accuracy=0.9239 ± 0.0280\n",
      "Depth=8: Accuracy=0.9239 ± 0.0406\n",
      "Depth=9: Accuracy=0.9239 ± 0.0406\n",
      "Depth=10: Accuracy=0.9239 ± 0.0406\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "depths = range(2, 11)\n",
    "means, stds = [], []\n",
    "\n",
    "for d in depths:\n",
    "    clf = DecisionTreeClassifier(max_depth=d, random_state=42)\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "    means.append(scores.mean())\n",
    "    stds.append(scores.std())\n",
    "\n",
    "for d, m, s in zip(depths, means, stds):\n",
    "    print(f\"Depth={d}: Accuracy={m:.4f} ± {s:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788e8bf7",
   "metadata": {},
   "source": [
    "The depth with the highest accuracy is depth = 3. At smaller depths the model is underfitting, meaning it’s too simple to capture all patterns in the data. Causing loss of accuracy.\n",
    "As the depth goes past 3, the accuracy actually decreases, meaning the model starts to overfit. Which means it learns too much detail from the training data and generalizes worse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
